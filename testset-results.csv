input,correct,expected,predicted,codebleu_score,bleu4_score
"def read(self, count=True, timeout=None, ignore_non_errors=True, ignore_timeouts=True): try: return self._read(count, timeout) except usb.USBError as e: <mask> DEBUG_COMM: log.info( ""read: e.errno=%s e.strerror=%s e.message=%s repr=%s"" % (e.errno, e.strerror, e.message, repr(e)) ) if ignore_timeouts and is_timeout(e): return [] if ignore_non_errors and is_noerr(e): return [] raise",False,if ignore_timeouts and is_timeout ( e ) :,if DEBUG_COMM :,0,3.47
"def _cache_mem(curr_out, prev_mem, mem_len, reuse_len=None): """"""cache hidden states into memory."""""" <mask> mem_len is None or mem_len == 0: return None else: if reuse_len is not None and reuse_len > 0: curr_out = curr_out[:reuse_len] if prev_mem is None: new_mem = curr_out[-mem_len:] else: new_mem = tf.concat([prev_mem, curr_out], 0)[-mem_len:] new_mem.stop_gradient = True return new_mem",True,if prev_mem is None :,if prev_mem is None :,0,100.0
def filtered(gen): for example in gen: example_len = length_fn(example) # Checking max length boundary. <mask> max_length is not None: if example_len > max_length: continue # Checking min length boundary. if min_length is not None: if example_len < min_length: continue # Within bounds. yield example,False,if example_len > max_length :,if max_length is not None :,0,20.61
"def search(self, query): # ""Search.ashx?query="" + query + filterVal <mask> not query: logger.debug(""Empty search query"") return [] logger.debug('Searching TuneIn for ""%s""' % query) args = ""&query="" + query search_results = self._tunein(""Search.ashx"", args) results = [] for item in self._flatten(search_results): if item.get(""type"", """") == ""audio"": # Only return stations self._stations[item[""guide_id""]] = item results.append(item) return results",True,"if item . get ( ""type"" , """" ) == ""audio"" :","if item . get ( ""type"" , """" ) == ""audio"" :",0,100.0
"def _check_script(self, script, directive): for var in compile_script(script): <mask> var.must_contain(""/""): # Skip variable checks return False if var.can_contain("".""): # Yay! Our variable can contain any symbols! reason = ( 'At least variable ""${var}"" can contain untrusted user input'.format( var=var.name ) ) self.add_issue(directive=[directive] + var.providers, reason=reason) return True return False",False,"if var . must_contain ( ""/"" ) :","if var . can_contain ( ""."" ) :",0,39.18
"def getAllDataLinkIDs(): linkDataIDs = set() dataType = _forestData.dataTypeBySocket for socketID, linkedIDs in _forestData.linkedSockets.items(): for linkedID in linkedIDs: <mask> socketID[1]: # check which one is origin/target linkDataIDs.add( (socketID, linkedID, dataType[socketID], dataType[linkedID]) ) else: linkDataIDs.add( (linkedID, socketID, dataType[linkedID], dataType[socketID]) ) return linkDataIDs",True,if socketID [ 1 ] :,if socketID [ 1 ] :,0,100.0
"def _stderr_supports_color(): try: <mask> hasattr(sys.stderr, ""isatty"") and sys.stderr.isatty(): if curses: curses.setupterm() if curses.tigetnum(""colors"") > 0: return True elif colorama: if sys.stderr is getattr( colorama.initialise, ""wrapped_stderr"", object() ): return True except Exception: # Very broad exception handling because it's always better to # fall back to non-colored logs than to break at startup. pass return False",True,"if curses . tigetnum ( ""colors"" ) > 0 :","if curses . tigetnum ( ""colors"" ) > 0 :",0,100.0
"def offsets(self): offsets = {} offset_so_far = 0 for name, ty in self.fields.items(): <mask> isinstance(ty, SimTypeBottom): l.warning( ""Found a bottom field in struct %s. Ignore and increment the offset using the default "" ""element size."", self.name, ) continue if not self._pack: align = ty.alignment if offset_so_far % align != 0: offset_so_far += align - offset_so_far % align offsets[name] = offset_so_far offset_so_far += ty.size // self._arch.byte_width return offsets",False,if offset_so_far % align != 0 :,"if isinstance ( ty , SimTypeBottom ) :",0,3.98
"def Restore(self): picker, obj = self._window, self._pObject value = obj.RestoreValue(PERSIST_FILEDIRPICKER_PATH) <mask> value is not None: if issubclass(picker.__class__, wx.FileDialog): if type(value) == list: value = value[-1] picker.SetPath(value) return True return False",False,if type ( value ) == list :,"if issubclass ( picker . __class__ , wx . FileDialog ) :",0,3.46
"def dt_s_tup_to_string(dt_s_tup): dt_string = dt_s_tup[0] # string for identifying the file to parse. <mask> dt_s_tup[1] > 0: # if there are seasons in the model if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string: dt_string = dt_string[:2] + ""s"" + dt_string[2:] else: dt_string = ""s"" + dt_string return dt_string",True,"if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string :","if ""co"" in dt_string or ""ci"" in dt_string or ""nc"" in dt_string :",0,100.0
"def writer(stream, items): sep = """" for item in items: stream.write(sep) sep = "" "" <mask> not isinstance(item, str): item = str(item) if not PY3K: if not isinstance(item, unicode): item = str(item) stream.write(item) stream.write(""\n"")",False,"if not isinstance ( item , str ) :",if not PY3K :,0,10.13
"def _get_result_keys(self, config): result_key = config.get(""result_key"") <mask> result_key is not None: if not isinstance(result_key, list): result_key = [result_key] result_key = [jmespath.compile(rk) for rk in result_key] return result_key",True,"if not isinstance ( result_key , list ) :","if not isinstance ( result_key , list ) :",0,100.0
"def _download_build_artifacts(self, build: Dict[str, Any]) -> None: arch = build[""arch_tag""] snap_build = self._lp_load_url(build[""self_link""]) urls = snap_build.getFileUrls() <mask> not urls: logger.error(f""Snap file not available for arch {arch!r}."") return for url in urls: file_name = _get_url_basename(url) self._download_file(url=url, dst=file_name) if file_name.endswith("".snap""): logger.info(f""Snapped {file_name}"") else: logger.info(f""Fetched {file_name}"")",True,"if file_name . endswith ( "".snap"" ) :","if file_name . endswith ( "".snap"" ) :",0,100.0
"def _add_custom_statement(self, custom_statements): <mask> custom_statements is None: return self.resource_policy[""Version""] = ""2012-10-17"" if self.resource_policy.get(""Statement"") is None: self.resource_policy[""Statement""] = custom_statements else: if not isinstance(custom_statements, list): custom_statements = [custom_statements] statement = self.resource_policy[""Statement""] if not isinstance(statement, list): statement = [statement] for s in custom_statements: if s not in statement: statement.append(s) self.resource_policy[""Statement""] = statement",False,if s not in statement :,"if not isinstance ( statement , list ) :",0,6.74
"def display_failures_for_single_test(result: TestResult) -> None: """"""Display a failure for a single method / endpoint."""""" display_subsection(result) checks = _get_unique_failures(result.checks) for idx, check in enumerate(checks, 1): message: Optional[str] <mask> check.message: message = f""{idx}. {check.message}"" else: message = None example = cast(Case, check.example) # filtered in `_get_unique_failures` display_example(example, check.name, message, result.seed) # Display every time except the last check if idx != len(checks): click.echo(""\n"")",True,if check . message :,if check . message :,0,100.0
"def build(opt): dpath = os.path.join(opt[""datapath""], ""qangaroo"") version = ""v1.1"" <mask> not build_data.built(dpath, version_string=version): print(""[building data: "" + dpath + ""]"") if build_data.built(dpath): # An older version exists, so remove these outdated files. build_data.remove_dir(dpath) build_data.make_dir(dpath) # Download the data. for downloadable_file in RESOURCES: downloadable_file.download_file(dpath) # Mark the data as built. build_data.mark_done(dpath, version_string=version)",True,if build_data . built ( dpath ) :,if build_data . built ( dpath ) :,0,100.0
"def call(self, step_input, states): new_states = [] for i in range(self.num_layers): out, new_state = self.lstm_cells[i](step_input, states[i]) step_input = ( layers.dropout( out, self.dropout_prob, dropout_implementation=""upscale_in_train"" ) <mask> self.dropout_prob > 0.0 else out ) new_states.append(new_state) return step_input, new_states",True,if self . dropout_prob > 0.0,if self . dropout_prob > 0.0,0,100.0
"def jupyter_progress_bar(min=0, max=1.0): """"""Returns an ipywidget progress bar or None <mask> we can't import it"""""" widgets = wandb.util.get_module(""ipywidgets"") try: if widgets is None: # TODO: this currently works in iPython but it's deprecated since 4.0 from IPython.html import widgets # type: ignore assert hasattr(widgets, ""VBox"") assert hasattr(widgets, ""Label"") assert hasattr(widgets, ""FloatProgress"") return ProgressWidget(widgets, min=min, max=max) except (ImportError, AssertionError): return None",True,if widgets is None :,if widgets is None :,0,100.0
"def _record_event(self, path, fsevent_handle, filename, events, error): with self.lock: self.events[path].append(events) <mask> events | pyuv.fs.UV_RENAME: if not os.path.exists(path): self.watches.pop(path).close()",False,if events | pyuv . fs . UV_RENAME :,if not os . path . exists ( path ) :,0,5.3
"def _get_v1_id_from_tags(self, tags_obj, tag): """"""Get image id from array of tags"""""" <mask> isinstance(tags_obj, dict): try: return tags_obj[tag] except KeyError: pass elif isinstance(tags_obj, []): try: for tag_dict in tags_obj: if tag_dict[""name""] == tag: return tag_dict[""layer""] except KeyError: pass return """"",True,"if tag_dict [ ""name"" ] == tag :","if tag_dict [ ""name"" ] == tag :",0,100.0
"def query_lister(domain, query="""", max_items=None, attr_names=None): more_results = True num_results = 0 next_token = None while more_results: rs = domain.connection.query_with_attributes( domain, query, attr_names, next_token=next_token ) for item in rs: <mask> max_items: if num_results == max_items: raise StopIteration yield item num_results += 1 next_token = rs.next_token more_results = next_token != None",True,if max_items :,if max_items :,0,100.0
"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) <mask> not is_callable(callbackfn): raise MakeError(""TypeError"", ""callbackfn must be a function"") _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: if array.has_property(unicode(k)): kValue = array.get(unicode(k)) if to_boolean(callbackfn.call(_this, (kValue, float(k), array))): res.append(kValue) k += 1 return args.space.ConstructArray(res)",True,if array . has_property ( unicode ( k ) ) :,if array . has_property ( unicode ( k ) ) :,0,100.0
"def every_one_is(self, dst): msg = ""all members of %r should be %r, but the %dth is %r"" for index, item in enumerate(self._src): <mask> self._range: if index < self._range[0] or index > self._range[1]: continue error = msg % (self._src, dst, index, item) if item != dst: raise AssertionError(error) return True",False,if item != dst :,if self . _range :,0,9.65
"def schedule_logger(job_id=None, delete=False): <mask> not job_id: return getLogger(""fate_flow_schedule"") else: if delete: with LoggerFactory.lock: try: for key in LoggerFactory.schedule_logger_dict.keys(): if job_id in key: del LoggerFactory.schedule_logger_dict[key] except: pass return True key = job_id + ""schedule"" if key in LoggerFactory.schedule_logger_dict: return LoggerFactory.schedule_logger_dict[key] return LoggerFactory.get_schedule_logger(job_id)",True,if job_id in key :,if job_id in key :,0,100.0
"def Tokenize(s): # type: (str) -> Iterator[Token] for item in TOKEN_RE.findall(s): # The type checker can't know the true type of item! item = cast(TupleStr4, item) <mask> item[0]: typ = ""number"" val = item[0] elif item[1]: typ = ""name"" val = item[1] elif item[2]: typ = item[2] val = item[2] elif item[3]: typ = item[3] val = item[3] yield Token(typ, val)",False,elif item [ 2 ] :,elif item [ 3 ] :,0,37.99
"def _read_data_from_all_categories(self, directory, config, categories): lines = [] for category in categories: data_file = os.path.join(directory, _DATASET_VERSION, category, config) <mask> os.path.exists(data_file): with open(data_file) as f: ls = f.read().split(""\n"") for l in ls[::-1]: if not l: ls.remove(l) lines.extend(ls) return lines",True,if os . path . exists ( data_file ) :,if os . path . exists ( data_file ) :,0,100.0
"def find_handlers(self, forms): handlers = {} for form in forms.itervalues(): for action_name, _action_label in form.actions: <mask> action_name not in handlers: handlers[action_name] = form else: raise HandlerError( ""More than one form defines the handler %s"" % action_name ) return handlers",True,if action_name not in handlers :,if action_name not in handlers :,0,100.0
"def get_story_task_completed_body(payload: Dict[str, Any]) -> Optional[str]: action = get_action_with_primary_id(payload) kwargs = { ""task_description"": action[""description""], } story_id = action[""story_id""] for ref in payload[""references""]: <mask> ref[""id""] == story_id: kwargs[""name_template""] = STORY_NAME_TEMPLATE.format( name=ref[""name""], app_url=ref[""app_url""], ) if action[""changes""][""complete""][""new""]: return STORY_TASK_COMPLETED_TEMPLATE.format(**kwargs) else: return None",True,"if ref [ ""id"" ] == story_id :","if ref [ ""id"" ] == story_id :",0,100.0
"def _create_valid_graph(graph): nodes = graph.nodes() for i in range(len(nodes)): for j in range(len(nodes)): <mask> i == j: continue edge = (nodes[i], nodes[j]) if graph.has_edge(edge): graph.del_edge(edge) graph.add_edge(edge, 1)",False,if i == j :,if graph . has_edge ( edge ) :,0,4.99
"def _post_order(op): <mask> isinstance(op, tvm.tir.Allocate): lift_stmt[-1].append(op) return op.body if isinstance(op, tvm.tir.AttrStmt): if op.attr_key == ""storage_scope"": lift_stmt[-1].append(op) return op.body if op.attr_key == ""virtual_thread"": return _merge_block(lift_stmt.pop() + [op], op.body) return op if isinstance(op, tvm.tir.For): return _merge_block(lift_stmt.pop() + [op], op.body) raise RuntimeError(""not reached"")",False,"if op . attr_key == ""storage_scope"" :","if op . attr_key == ""virtual_thread"" :",0,65.92
"def format_lazy_import(names): """"""Formats lazy import lines"""""" lines = """" for _, name, asname in names: pkg, _, _ = name.partition(""."") <mask> asname is None: line = ""{pkg} = _LazyModule.load({pkg!r}, {mod!r})\n"" else: line = ""{asname} = _LazyModule.load({pkg!r}, {mod!r}, {asname!r})\n"" lines += line.format(pkg=pkg, mod=name, asname=asname) return lines",True,if asname is None :,if asname is None :,0,100.0
"def evaluateWord(self, argument): wildcard_count = argument[0].count(""*"") <mask> wildcard_count > 0: if wildcard_count == 1 and argument[0].startswith(""*""): return self.GetWordWildcard(argument[0][1:], method=""endswith"") if wildcard_count == 1 and argument[0].endswith(""*""): return self.GetWordWildcard(argument[0][:-1], method=""startswith"") else: _regex = argument[0].replace(""*"", "".+"") matched = False for w in self.words: matched = bool(re.search(_regex, w)) if matched: break return matched return self.GetWord(argument[0])",True,if matched :,if matched :,0,0.0
"def setup(self, ir: ""IR"", aconf: Config) -> bool: <mask> self.kind == ""ConsulResolver"": self.resolve_with = ""consul"" if not self.get(""datacenter""): self.post_error(""ConsulResolver is required to have a datacenter"") return False elif self.kind == ""KubernetesServiceResolver"": self.resolve_with = ""k8s"" elif self.kind == ""KubernetesEndpointResolver"": self.resolve_with = ""k8s"" else: self.post_error(f""Resolver kind {self.kind} unknown"") return False return True",False,"if not self . get ( ""datacenter"" ) :","elif self . kind == ""KubernetesEndpointResolver"" :",0,9.55
"def get_success_url(self): """"""Continue to the flow index or redirect according `?back` parameter."""""" <mask> ""back"" in self.request.GET: back_url = self.request.GET[""back""] if not is_safe_url(url=back_url, allowed_hosts={self.request.get_host()}): back_url = ""/"" return back_url return reverse(self.success_url)",True,"if not is_safe_url ( url = back_url , allowed_hosts = { self . request . get_host ( ) } ) :","if not is_safe_url ( url = back_url , allowed_hosts = { self . request . get_host ( ) } ) :",0,100.0
"def download_main( download, download_playlist, urls, playlist, output_dir, merge, info_only ): for url in urls: <mask> url.startswith(""https://""): url = url[8:] if not url.startswith(""http://""): url = ""http://"" + url if playlist: download_playlist( url, output_dir=output_dir, merge=merge, info_only=info_only ) else: download(url, output_dir=output_dir, merge=merge, info_only=info_only)",False,"if not url . startswith ( ""http://"" ) :","if url . startswith ( ""https://"" ) :",0,63.44
"def __str__(self): buf = [""""] <mask> self.fileName: buf.append(self.fileName + "":"") if self.line != -1: if not self.fileName: buf.append(""line "") buf.append(str(self.line)) if self.column != -1: buf.append("":"" + str(self.column)) buf.append("":"") buf.append("" "") return str("""").join(buf)",True,if not self . fileName :,if not self . fileName :,0,100.0
"def parse_bash_set_output(output): """"""Parse Bash-like 'set' output"""""" <mask> not sys.platform.startswith(""win""): # Replace ""\""-continued lines in *Linux* environment dumps. # Cannot do this on Windows because a ""\"" at the end of the # line does not imply a continuation. output = output.replace(""\\\n"", """") environ = {} for line in output.splitlines(0): line = line.rstrip() if not line: continue # skip black lines item = _ParseBashEnvStr(line) if item: environ[item[0]] = item[1] return environ",False,if item :,if not line :,0,19.0
"def remove_selected(self): """"""Removes selected items from list."""""" to_delete = [] for i in range(len(self)): <mask> self[i].selected: to_delete.append(i) to_delete.reverse() for i in to_delete: self.pop(i) if len(to_delete) > 0: first_to_delete = to_delete[-1] if first_to_delete == 0 and len(self) > 0: self[0].selected = True elif first_to_delete > 0: self[first_to_delete - 1].selected = True",True,if first_to_delete == 0 and len ( self ) > 0 :,if first_to_delete == 0 and len ( self ) > 0 :,0,100.0
"def update(self, update_tracks=True): self.enable_update_metadata_images(False) old_album_title = self.metadata[""album""] self.metadata[""album""] = config.setting[""nat_name""] for track in self.tracks: <mask> old_album_title == track.metadata[""album""]: track.metadata[""album""] = self.metadata[""album""] for file in track.linked_files: track.update_file_metadata(file) self.enable_update_metadata_images(True) super().update(update_tracks)",True,"if old_album_title == track . metadata [ ""album"" ] :","if old_album_title == track . metadata [ ""album"" ] :",0,100.0
"def on_input(self, target, message): <mask> message.strip() == """": self.panel(""No commit message provided"") return if target: command = [""git"", ""add""] if target == ""*"": command.append(""--all"") else: command.extend((""--"", target)) self.run_command(command, functools.partial(self.add_done, message)) else: self.add_done(message, """")",True,"if target == ""*"" :","if target == ""*"" :",0,100.0
"def go_to_last_edit_location(self): <mask> self.last_edit_cursor_pos is not None: filename, position = self.last_edit_cursor_pos if not osp.isfile(filename): self.last_edit_cursor_pos = None return else: self.load(filename) editor = self.get_current_editor() if position < editor.document().characterCount(): editor.set_cursor_position(position)",False,if not osp . isfile ( filename ) :,if position < editor . document ( ) . characterCount ( ) :,0,7.77
"def returnByType(self, results): new_results = {} for r in results: type_name = r.get(""type"", ""movie"") + ""s"" <mask> type_name not in new_results: new_results[type_name] = [] new_results[type_name].append(r) # Combine movies, needs a cleaner way.. if ""movies"" in new_results: new_results[""movies""] = self.combineOnIMDB(new_results[""movies""]) return new_results",True,if type_name not in new_results :,if type_name not in new_results :,0,100.0
"def cache_sns_topics_across_accounts() -> bool: function: str = f""{__name__}.{sys._getframe().f_code.co_name}"" # First, get list of accounts accounts_d: list = async_to_sync(get_account_id_to_name_mapping)() for account_id in accounts_d.keys(): <mask> config.get(""environment"") == ""prod"": cache_sns_topics_for_account.delay(account_id) else: if account_id in config.get(""celery.test_account_ids"", []): cache_sns_topics_for_account.delay(account_id) stats.count(f""{function}.success"") return True",False,"if account_id in config . get ( ""celery.test_account_ids"" , [ ] ) :","if config . get ( ""environment"" ) == ""prod"" :",0,16.72
"def get(self, subject, topic): """"""Handles GET requests."""""" <mask> subject in feconf.AVAILABLE_LANDING_PAGES: if topic in feconf.AVAILABLE_LANDING_PAGES[subject]: self.render_template(""topic-landing-page.mainpage.html"") else: raise self.PageNotFoundException else: raise self.PageNotFoundException",True,if topic in feconf . AVAILABLE_LANDING_PAGES [ subject ] :,if topic in feconf . AVAILABLE_LANDING_PAGES [ subject ] :,0,100.0
"def callback(compiled): <mask> destpath is None: logger.show_tabulated( ""Compiled"", showpath(codepath), ""without writing to file."" ) else: with univ_open(destpath, ""w"") as opened: writefile(opened, compiled) logger.show_tabulated(""Compiled to"", showpath(destpath), ""."") if self.show: print(compiled) if run: if destpath is None: self.execute(compiled, path=codepath, allow_show=False) else: self.execute_file(destpath)",False,if destpath is None :,if self . show :,0,12.7
"def _find_start_index(self, string, start, end): while True: index = string.find(""{"", start, end) - 1 <mask> index < 0: return -1 if self._start_index_is_ok(string, index): return index start = index + 2",True,"if self . _start_index_is_ok ( string , index ) :","if self . _start_index_is_ok ( string , index ) :",0,100.0
"def _get_nlu_target_format(export_path: Text) -> Text: guessed_format = loading.guess_format(export_path) <mask> guessed_format not in {MARKDOWN, RASA, RASA_YAML}: if rasa.shared.data.is_likely_json_file(export_path): guessed_format = RASA elif rasa.shared.data.is_likely_markdown_file(export_path): guessed_format = MARKDOWN elif rasa.shared.data.is_likely_yaml_file(export_path): guessed_format = RASA_YAML return guessed_format",True,elif rasa . shared . data . is_likely_yaml_file ( export_path ) :,elif rasa . shared . data . is_likely_yaml_file ( export_path ) :,0,100.0
"def moveToThreadNext(self): """"""Move a position to threadNext position."""""" p = self <mask> p.v: if p.v.children: p.moveToFirstChild() elif p.hasNext(): p.moveToNext() else: p.moveToParent() while p: if p.hasNext(): p.moveToNext() break # found p.moveToParent() # not found. return p",False,if p . hasNext ( ) :,if p . v . children :,0,26.27
"def copy_attributes(info_add, obj, name_fmt, attributes, formatter=None): for attr in attributes: value = getattr(obj, attr, None) <mask> value is None: continue name = name_fmt % attr if formatter is not None: value = formatter(attr, value) info_add(name, value)",True,if value is None :,if value is None :,0,100.0
"def getElement(self, aboutUri, namespace, name): for desc in self.rdfRoot.getElementsByTagNameNS(RDF_NAMESPACE, ""Description""): <mask> desc.getAttributeNS(RDF_NAMESPACE, ""about"") == aboutUri: attr = desc.getAttributeNodeNS(namespace, name) if attr != None: yield attr for element in desc.getElementsByTagNameNS(namespace, name): yield element",False,"if desc . getAttributeNS ( RDF_NAMESPACE , ""about"" ) == aboutUri :",if attr != None :,0,1.71
